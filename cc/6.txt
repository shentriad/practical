Implementing Hadoop & Map Reduce Using Cloudera.--Download and extract Cloudera VM files| Open 
Oracle VirtualBox| Select File > Import Appliance| Select the downloaded Cloudera VM| Set RAM as 8GB| Set CPU 
count to 2| Start the VM| Start Eclipse| Click File > New > Java Project| Right-click project > Build Path > Configure 
Build Path| Click Add External JARS, go to the following file location: File System > usr > lib, and select the required 
JAR files (IMPORTANT: location is different here)| Right-click src > New > Package| Right-click package > New > 
Class| Write the required code (down here)| Right-click project > Export > Select JAR| Select “Extract source 
files…” and specify the location| JAR file was created| Initially, no files in Hadoop FS| Make a directory| Create a file 
and write some content in it| Put the created file in Hadoop file system| Check the created file| Run the JAR file| 
Check the output directory| Check the output| Perform MapReduce with two files| Create another file| Put the file in 
Hadoop file system| Run the jar command| Check the output| Open browser| Username and Password - cloudera| 
Click on Menu icon on the left side and then select Files| Navigate to Input directory and locate File1| Check Output 
Directory|





//Mapper
package wc;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase; 
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector; 
import org.apache.hadoop.mapred.Reporter;
public class WCMapper extends MapReduceBase implements Mapper<LongWritable,Text,Text,IntWritable>
{
public void map(LongWritable key, Text value,OutputCollector<Text,IntWritable> output,
Reporter reporter) throws IOException
{
String line = value.toString(); 
String tokenizer[] = line.split(""); 
for(String SingleChar : tokenizer){
Text charKey = new Text(SingleChar); 
IntWritable One = new IntWritable(1); 
output.collect(charKey, One);
}
}
}

//Reducer
package wc;
import java.io.IOException; 
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase; 
import org.apache.hadoop.mapred.OutputCollector; 
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

public class WCReducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable>
{
public void reduce(Text key, Iterator<IntWritable>values,OutputCollector<Text,IntWritable> output,
Reporter reporter) throws IOException
{
int sum=0;
while (values.hasNext())
{
sum+=values.next().get();
}
output.collect(key,newIntWritable(sum));
}
}

//driver
package wc;

import java.io.IOException;
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat; 
import org.apache.hadoop.mapred.FileOutputFormat; 
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat; 
import org.apache.hadoop.mapred.TextOutputFormat;

public class WCDriver
{
public static void main(String[] args) throws IOException
{
JobConf conf = new JobConf(WCDriver.class); 
conf.setJobName("CharCount"); 
conf.setOutputKeyClass(Text.class); 
conf.setOutputValueClass(IntWritable.class); 
conf.setMapperClass(WCMapper.class); 
conf.setCombinerClass(WCReducer.class); 
conf.setReducerClass(WCReducer.class);

conf.setInputFormat(TextInputFormat.class); 
conf.setOutputFormat(TextOutputFormat.class); 
FileInputFormat.setInputPaths(conf,new Path(args[0])); 
FileOutputFormat.setOutputPath(conf,new Path(args[1])); 
JobClient.runJob(conf);
}
}
